{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IX.2424 Introduction to Artificial Intelligence Project - CHEUNG, Hoi Ching 63369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"~/Documents/ISEP/AI_intro/latestdata.csv\",sep=\",\",index_col=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A: Analysis the dataset\n",
    "\n",
    "In order to analyse the dataset, you have to extract some statistical information from the given dataset, for example: the type of data, the missing values, outliers, the correlation between variables, etc. If there are missing values, you can replace them by the mean, median or mode of the concerning variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df.shape)\n",
    "\n",
    "df_processed = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill empty records, drop more than half of the records are empty\n",
    "\n",
    "def fill_mean(col):\n",
    "    return col.fillna(col.mean())\n",
    "def fill_median(col):\n",
    "    return col.fillna(col.median())\n",
    "def fill_mode(col):\n",
    "    mode = col.mode()\n",
    "    if not mode.empty:\n",
    "        return col.fillna(mode[0])\n",
    "    else:\n",
    "        return col\n",
    "    \n",
    "def fill_null(df, threshold=0.9):\n",
    "    empty_cols = []\n",
    "    for column in df.columns:\n",
    "        missing_fraction = df[column].isnull().mean()\n",
    "        if missing_fraction < threshold:\n",
    "            if df[column].dtype == 'float64':\n",
    "                df[column] = fill_median(df[column])\n",
    "            else:\n",
    "                df[column] = fill_mode(df[column])\n",
    "        else:\n",
    "            empty_cols.append(column)\n",
    "    df.drop(columns=empty_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_processed = fill_null(df_processed)\n",
    "\n",
    "print(df_processed.dtypes)\n",
    "print(df_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unecessary columns\n",
    "\n",
    "def drop_if_exist(col, df):\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=col, inplace=True)\n",
    "\n",
    "drop_if_exist('source', df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert age\n",
    "\n",
    "def age_mean(age):\n",
    "    if isinstance(age, str) and '-' in age:\n",
    "        age_range = age.split('-')\n",
    "        age_start = age_range[0]\n",
    "        age_end = age_range[1]\n",
    "        if age_start == '':\n",
    "            return float(age_end)\n",
    "        elif age_end == '':\n",
    "            return float(age_start)\n",
    "        else:\n",
    "            age_start = int(age_start)\n",
    "            age_end = int(age_end)\n",
    "            mean_age = (age_start + age_end) / 2\n",
    "            return float(mean_age)\n",
    "    else:\n",
    "        try:\n",
    "            return float(age)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "df_processed['age'] = df_processed['age'].apply(age_mean)\n",
    "df_processed['age'] = fill_median(df_processed['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sex\n",
    "\n",
    "df_processed['sex'] = df_processed['sex'].map({'male': 1, 'female': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date\n",
    "\n",
    "def date_mid(date):\n",
    "    if isinstance(date, str) and '-' in date:\n",
    "        date = date.strip()\n",
    "        date_range = date.split('-')\n",
    "        date_start = date_range[0].strip()\n",
    "        date_end = date_range[1].strip()\n",
    "        if date_start == \"\":\n",
    "            return pd.to_datetime(date_end, format='%d.%m.%Y')\n",
    "        elif date_end == \"\":\n",
    "            return pd.to_datetime(date_start, format='%d.%m.%Y')\n",
    "        else:\n",
    "            date_start = pd.to_datetime(date_start, format='%d.%m.%Y')\n",
    "            date_end = pd.to_datetime(date_end, format='%d.%m.%Y')\n",
    "            date_mid = date_start + (date_end - date_start) / 2\n",
    "            return date_mid\n",
    "    else:\n",
    "        try:\n",
    "            date = pd.to_datetime(date, format='%d.%m.%Y')\n",
    "            return date\n",
    "        except ValueError:\n",
    "            return date.median()\n",
    "\n",
    "df_processed['date_confirmation'] = df_processed['date_confirmation'].apply(date_mid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert outcome\n",
    "\n",
    "outcome_mapping = {\n",
    "    'critical condition, intubated as of 14.02.2020': 'Critical',\n",
    "    'death': 'Dead',\n",
    "    'discharge': 'Discharged',\n",
    "    'Hospitalized': 'Hospitalized',\n",
    "    'discharged': 'Discharged',\n",
    "    'Discharged': 'Discharged',\n",
    "    'Discharged from hospital': 'Discharged',\n",
    "    'not hospitalized': 'Not Hospitalized',\n",
    "    'recovered': 'Recovered',\n",
    "    'recovering at home 03.03.2020': 'Recovering at Home',\n",
    "    'released from quarantine': 'Released from Quarantine',\n",
    "    'severe': 'Severe',\n",
    "    'stable': 'Stable',\n",
    "    'died': 'Dead',\n",
    "    'Death': 'Dead',\n",
    "    'dead': 'Dead',\n",
    "    'Symptoms only improved with cough. Currently hospitalized for follow-up.': 'Hospitalized',\n",
    "    'treated in an intensive care unit (14.02.2020)': 'ICU',\n",
    "    'Alive': 'Alive',\n",
    "    'Dead': 'Dead',\n",
    "    'Recovered': 'Recovered',\n",
    "    'Stable': 'Stable',\n",
    "    'Died': 'Dead',\n",
    "    'Deceased': 'Dead',\n",
    "    'stable condition': 'Stable',\n",
    "    'Under treatment': 'Under Treatment',\n",
    "    'Critical condition': 'Critical',\n",
    "    'Receiving Treatment': 'Under Treatment',\n",
    "    'severe illness': 'Severe',\n",
    "    'unstable': 'Unstable',\n",
    "    'critical condition': 'Critical',\n",
    "    'Migrated': 'Migrated',\n",
    "    'Migrated_Other': 'Migrated',\n",
    "    'https://www.mspbs.gov.py/covid-19.php': 'Unknown'\n",
    "}\n",
    "\n",
    "serverity_mapping = {\n",
    "    'Released from Quarantine': 0,\n",
    "    'Alive': 1,\n",
    "    'Recovered': 2,\n",
    "    'Recovering at Home': 3,\n",
    "    'Not Hospitalized': 4,\n",
    "    'Discharged': 5,\n",
    "    'Migrated': 6,\n",
    "    'Under Treatment': 7,\n",
    "    'Hospitalized': 8,\n",
    "    'Stable': 9,\n",
    "    'Unstable': 10,\n",
    "    'Severe': 11,\n",
    "    'ICU': 12,\n",
    "    'Critical': 13,\n",
    "    'Dead': 14,\n",
    "    'Unknown': 8   \n",
    "}\n",
    "\n",
    "\n",
    "df_processed['outcome'] = df_processed['outcome'].map(outcome_mapping)\n",
    "print(df_processed['outcome'].unique())\n",
    "print(df_processed['outcome'].mode())\n",
    "df_processed['outcome'] = df_processed['outcome'].map(serverity_mapping)\n",
    "print(df_processed['outcome'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the rest\n",
    "df_processed['city'].str.lower()\n",
    "df_processed['city'] = df_processed['city'].astype('category').cat.codes\n",
    "print(df_processed['city'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['province'] = df_processed['province'].str.lower()\n",
    "df_processed['province'] = df_processed['province'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['country'] = df_processed['country'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['geo_resolution'] = df_processed['geo_resolution'].astype('category').cat.codes\n",
    "print(df_processed['geo_resolution'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['admin2'] = df_processed['admin2'].astype('category').cat.codes\n",
    "df_processed['admin1'] = df_processed['admin1'].astype('category').cat.codes\n",
    "df_processed['country_new'] = df_processed['country_new'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['data_moderator_initials'] = df_processed['data_moderator_initials'].str.upper()\n",
    "df_processed['data_moderator_initials'] = df_processed['data_moderator_initials'].astype('category').cat.codes\n",
    "print(df_processed['data_moderator_initials'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_processed.shape)\n",
    "print(df_processed.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the correlations between the variables. Which variables are most correlated with the target (outcome)? Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_processed.corr()\n",
    "target_correlation = correlation_matrix['outcome'].sort_values(ascending=False)\n",
    "print(target_correlation)\n",
    "#print(abs(target_correlation).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables that are most correlated with the target are 'longitude', 'date_confirmation', and 'geo_resolution'.\n",
    "\n",
    "longitude (-0.247107): This negative correlation suggests that as the longitude increases (moving eastward), the outcome variable tends to decrease. As the outcome variable is mapped according to severity with a higher number being more severe, this suggests that the outcome is less severe in eastern countries and vice versa.\n",
    "\n",
    "date_confirmation (-0.197443): This negative correlation indicates that as the date of confirmation becomes more recent, the outcome tends to decrease. This might suggest improvements in outcomes over time, possibly due to better treatments or interventions, and the weaker viral power of COVID19 viruses.\n",
    "\n",
    "geo_resolution (-0.165848): This negative correlation suggests that different levels of geographical resolution are inversely related to the outcome. This could imply that outcomes vary significantly with the granularity of location data, potentially due to local healthcare infrastructure or reporting practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the dataset using scatter and analyze the obtained result. Use the PCA (Principal Component Analysis) to project the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_processed.copy()\n",
    "df_plot['date_confirmation'] = df_plot['date_confirmation'].astype('int64') / 10**9 \n",
    "X = df_plot.drop(columns=['outcome'])   \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df.index = df_processed.index\n",
    "pca_df['outcome'] = df_processed['outcome']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['outcome'], cmap='viridis', alpha=0.3)\n",
    "plt.title('PCA of Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(scatter, label='Outcome')\n",
    "plt.show()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f'Explained variance by PC1: {explained_variance[0]:.2f}')\n",
    "print(f'Explained variance by PC2: {explained_variance[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B : Bayets Nets\n",
    "\n",
    "1. What is the probability for a person to have symptoms of COVID-19 (symptom onset=date) if this person visited Wuhan (visiting Wuhan = 1)? Consider that (symptom onset=N/A) means that the patient is asymp- tomatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob = df.copy()\n",
    "df_prob['travel_history_location'] = df_prob['travel_history_location'].str.lower()\n",
    "wuhan_visitors = df_prob[df_prob['travel_history_location'].str.contains('wuhan', na=False) | \n",
    "                      (df_prob['lives_in_Wuhan'] == '1') | \n",
    "                      (df_prob['travel_history_binary'] == '1')]\n",
    "\n",
    "total_wuhan_visitors = len(wuhan_visitors)\n",
    "\n",
    "symptomatic_wuhan_visitors = wuhan_visitors['date_onset_symptoms'].notna().sum()\n",
    "\n",
    "probability1 = symptomatic_wuhan_visitors / total_wuhan_visitors\n",
    "\n",
    "print(probability1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the probability for a person to be a true patient if this person has symptoms of COVID-19 (symptom onset=date) and this person visited Wuhan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob2 = df_prob[(df_prob['date_onset_symptoms'].notnull()) & (df_prob['travel_history_location'].str.contains('wuhan', na=False) | \n",
    "                      (df_prob['lives_in_Wuhan'] == '1') | \n",
    "                      (df_prob['travel_history_binary'] == '1'))]\n",
    "\n",
    "total_rows = df_prob2.shape[0]\n",
    "print(total_rows)\n",
    "true_patients = df_prob2['date_confirmation'].notnull().sum()\n",
    "\n",
    "probability2 = true_patients / total_rows\n",
    "\n",
    "print(probability2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the probability for a person to death if this person visited Wuhan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob['outcome'] = df_prob['outcome'].map(outcome_mapping)\n",
    "df_prob['outcome'] = fill_mode(df_prob['outcome'])\n",
    "dead_count = (wuhan_visitors['outcome'] == 'Dead').sum()\n",
    "probability3 = dead_count / total_wuhan_visitors\n",
    "print(probability3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Estimate the average recovery interval for a patient if this person visited Wuhan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wuhan_visitors['date_onset_symptoms'] = date_mid(wuhan_visitors['date_onset_symptoms'])\n",
    "wuhan_visitors['date_death_or_discharge'] = date_mid(wuhan_visitors['date_death_or_discharge'])\n",
    "wuhan_visitors = wuhan_visitors[wuhan_visitors['date_onset_symptoms'].notnull() & wuhan_visitors['date_death_or_discharge'].notnull()]\n",
    "\n",
    "wuhan_visitors['recovery_interval'] = wuhan_visitors['date_death_or_discharge'] - wuhan_visitors['date_onset_symptoms']\n",
    "\n",
    "wuhan_patients = wuhan_visitors[wuhan_visitors['recovery_interval'] >= pd.Timedelta(0)]\n",
    "\n",
    "average_recovery_interval = wuhan_patients['recovery_interval'].mean()\n",
    "\n",
    "print(\"Average recovery interval for patients who visited Wuhan:\", average_recovery_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C: Machine Learning\n",
    "\n",
    "In this part, we use a machine learning method in order to predict the outcome: patients outcome as either ’died’ or ’discharged’ from hospital. You can use the K-Nearest Neighbours (K-NN) or Bayes Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The obtained results should be validated using some external indexes as prediction error (Confusion matrix and Accuracy) or others as Recall, F- Measure, etc. The obtained results should be analyzed in the report and provide a solution to ameliorate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_processed['outcome']\n",
    "for col in df_processed.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the Regression to predict the age of persons based on other variables. You have the choice on these explanatory variables? How you choose these variables? Compute the quality of the prediction using MSE error (Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_reg = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_reg)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply a clustering method (K-means) on the dataset to segment the persons into different clusters. Use the Silhouette index to find out the best number of clusters. Plot the results using scatter to visually analyze the clustering structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2, 11)\n",
    "silhouette_scores = []\n",
    "k_values = []\n",
    "data_scaled = X\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(data_scaled)\n",
    "    score = silhouette_score(data_scaled, cluster_labels)\n",
    "    silhouette_scores.append(score)\n",
    "    k_values.append(k)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Different k Values')\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(data_scaled)\n",
    "data['Cluster'] = cluster_labels\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "principal_df['Cluster'] = cluster_labels\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=principal_df, palette='viridis')\n",
    "plt.title('K-means Clustering with Optimal Number of Clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D: Improving the results and Theoretical formalism\n",
    "\n",
    "1. The data is unbalanced. You can balance it by reducing randomly the majority class. Assume that you extract randomly samples that are balanced. How will the prediction results change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing the dataset can improve the precision and recall for the minority class because the model will have more balanced exposure to both classes during training.\n",
    "\n",
    "The F1 score is likely to improve for the minority class, reflecting better overall performance in predicting the minority class.\n",
    "\n",
    "Balancing the dataset also helps reduce the bias, leading to a more equitable treatment of both classes.\n",
    "\n",
    "There is a possible Decrease in Overall Accuracy.\n",
    "\n",
    "By reducing the majority class, the dataset size decreases, which could lead to overfitting, especially if the remaining data is not representative of the entire population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How can you better manage the missing values?\n",
    "\n",
    "We can replace them with a constant value, or use the KNN algorithm to predict their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To find the best parameters for the models, the Grid-search algorithm can be used which is available in scikit-learn library. Explain the algorithm and use it for the learning models to find the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search works by exhaustively searching through a specified parameter grid (combinations of hyperparameters) to determine the optimal combination for a given model. This is achieved by training and evaluating the model for each combination of hyperparameters using cross-validation, and selecting the combination that yields the best performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "features = df.columns\n",
    "target = 'outcome'\n",
    "data = data[features + [target]].dropna()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "if data['age'].dtype == 'object':\n",
    "    data['age'] = label_encoder.fit_transform(data['age'])\n",
    "data['outcome'] = label_encoder.fit_transform(data['outcome'])\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "numeric_features = ['age', 'latitude', 'longitude']\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X[numeric_features] = knn_imputer.fit_transform(X[numeric_features])\n",
    "\n",
    "categorical_features = ['chronic_disease_binary', 'travel_history_binary']\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_features] = mode_imputer.fit_transform(X[categorical_features])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "best_rf = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\")\n",
    "print(roc_auc_score(y_test, best_rf.predict_proba(X_test_scaled)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Give the algorithmically (mathematical) formalism of the method which gives the best results. Explain all the parameters of the used method and their impact on the results. Some comparison with public results should be made to conclude the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest (RF) algorithm is an ensemble learning method for classification (and regression) that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. \n",
    "Key Parameters and Their Impact\n",
    "n_estimators: The number of trees in the forest.\n",
    "\n",
    "Impact: Increasing the number of trees generally improves performance and reduces overfitting, but with diminishing returns. More trees also increase computational cost.\n",
    "max_depth: The maximum depth of the trees.\n",
    "\n",
    "Impact: Limiting the depth can reduce overfitting, especially with noisy data. Too shallow trees may underfit.\n",
    "min_samples_split: The minimum number of samples required to split an internal node.\n",
    "\n",
    "Impact: A higher value prevents the model from learning overly specific patterns, which reduces overfitting but may lead to underfitting.\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "Impact: Setting this to a higher number can smooth the model, preventing it from learning spurious patterns.\n",
    "max_features: The number of features to consider when looking for the best split.\n",
    "\n",
    "Impact: Reducing the number of features considered for each split can help to reduce overfitting and improve generalization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3Env",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
